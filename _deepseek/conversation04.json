[
  {
    "question": "smart_monitor.pyPY 35.05KBI messed it up. Please make the changes for me.",
    "answer": "Here's the complete, corrected smart_monitor.py file:pythonCopyDownload# smart_monitor.py\nimport asyncio\nimport os\nimport signal\nimport sys\nfrom datetime import datetime, timedelta\nfrom telethon import TelegramClient, events\nfrom telethon.tl.types import MessageMediaPhoto\nimport sqlite3\nimport pytesseract\nfrom PIL import Image, ImageEnhance, ImageFilter, ImageOps\nimport io\nimport hashlib\nimport re\nimport pandas as pd\nimport numpy as np\nfrom dotenv import load_dotenv\nimport logging\nimport aiofiles\nimport json\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('data/monitor.log'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\nload_dotenv()\n\nclass SmartMonitor:\n    def __init__(self):\n        self.api_id = os.getenv('TELEGRAM_API_ID')\n        self.api_hash = os.getenv('TELEGRAM_API_HASH')\n        self.bot_token = os.getenv('TELEGRAM_BOT_TOKEN')\n        self.report_channel = os.getenv('REPORT_CHANNEL', '@TheLineWeaverReports')\n        \n        # We'll get the source channel from dialogs directly\n        self.source_channel_entity = None\n        \n        # Validate required environment variables\n        if not all([self.api_id, self.api_hash]):\n            raise ValueError(\"Missing required environment variables: TELEGRAM_API_ID, TELEGRAM_API_HASH\")\n        \n        # Convert API ID to integer\n        self.api_id = int(self.api_id)\n        \n        self.client = None\n        self.bot_client = None\n        self.is_running = True\n        self.shutdown_event = asyncio.Event()\n        self.last_report_time = datetime.now()\n        self.report_interval = timedelta(minutes=20)\n        self.new_picks_count = 0\n        self.total_picks_processed = 0\n        self.last_processed_message_id = 0\n        \n        # Known cappers and watermark patterns for CAPPERS FREE\n        self.known_cappers = [\n            'KingCap', 'PorterPicks', 'LockLineLarry', 'VegasVic', 'SharpShooter',\n            'MoneylineMike', 'SpreadSpecialist', 'TheProphet', 'WinWithWill',\n            'BettingBaron', 'CapperChris', 'DimeDave', 'EliteEdge', 'FadeFinder'\n        ]\n        \n        # Message filtering patterns\n        self.exclude_patterns = [\n            # Capper watermarks and tags\n            r'(?i)dm‚û°Ô∏è@cappersfree‚úÖ',\n            r'(?i)@cappersfree',\n            r'(?i)cappersfree',\n            r'(?i)dm\\s*@',\n            r'(?i)‚û°Ô∏è@',\n            r'(?i)‚úÖ',\n            r'(?i)üÜì',\n            r'(?i)üíé',\n            \n            # Ads and spam\n            r'(?i)buy now',\n            r'(?i)limited time',\n            r'(?i)discount',\n            r'(?i)promo',\n            r'(?i)offer',\n            r'(?i)deal',\n            r'(?i)sale',\n            r'(?i)shop now',\n            r'(?i)click here',\n            r'(?i)sign up',\n            r'(?i)subscribe',\n        ]\n        \n        # Performance tracking\n        self.performance_stats = {\n            'images_processed': 0,\n            'ocr_success_rate': 0,\n            'avg_processing_time': 0,\n            'errors_count': 0,\n            'messages_filtered': 0\n        }\n        \n        # Create data directory\n        os.makedirs('data/temp', exist_ok=True)\n        os.makedirs('data/logs', exist_ok=True)\n        self.init_database()\n        self.load_state()\n\n    def init_database(self):\n        \"\"\"Initialize database with latest schema\"\"\"\n        os.makedirs('data', exist_ok=True)\n        conn = sqlite3.connect('data/sports_picks.db')\n        cursor = conn.cursor()\n        \n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS picks (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                handicapper TEXT,\n                sport TEXT,\n                teams TEXT,\n                pick_type TEXT,\n                pick_value TEXT,\n                confidence REAL,\n                extracted_date TEXT,\n                source_channel TEXT,\n                image_hash TEXT UNIQUE,\n                ocr_confidence REAL DEFAULT 0.0,\n                message_id INTEGER DEFAULT 0,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n        \n        # Create filtered messages table to track excluded content\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS filtered_messages (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                message_id INTEGER,\n                reason TEXT,\n                filtered_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n        \n        # Create index for better performance\n        cursor.execute('''\n            CREATE INDEX IF NOT EXISTS idx_created_at \n            ON picks(created_at)\n        ''')\n        \n        cursor.execute('''\n            CREATE INDEX IF NOT EXISTS idx_sport_teams \n            ON picks(sport, teams)\n        ''')\n        \n        conn.commit()\n        conn.close()\n        logger.info(\"‚úÖ Database initialized with optimized schema\")\n\n    def load_state(self):\n        \"\"\"Load previous state from file\"\"\"\n        try:\n            state_file = 'data/monitor_state.json'\n            if os.path.exists(state_file):\n                with open(state_file, 'r') as f:\n                    state = json.load(f)\n                    self.last_processed_message_id = state.get('last_processed_message_id', 0)\n                    logger.info(f\"üì• Loaded state: last message ID {self.last_processed_message_id}\")\n        except Exception as e:\n            logger.warning(f\"Could not load state: {e}\")\n\n    def save_state(self):\n        \"\"\"Save current state to file\"\"\"\n        try:\n            state_file = 'data/monitor_state.json'\n            state = {\n                'last_processed_message_id': self.last_processed_message_id,\n                'last_save_time': datetime.now().isoformat(),\n                'total_picks_processed': self.total_picks_processed,\n                'performance_stats': self.performance_stats\n            }\n            with open(state_file, 'w') as f:\n                json.dump(state, f, indent=2)\n        except Exception as e:\n            logger.warning(f\"Could not save state: {e}\")\n\n    def should_filter_message(self, text):\n        \"\"\"Check if message should be filtered as ad/junk\"\"\"\n        if not text or not text.strip():\n            return False, \"Empty text\"\n            \n        text_lower = text.lower().strip()\n        \n        # Check against exclusion patterns\n        for pattern in self.exclude_patterns:\n            if re.search(pattern, text_lower):\n                return True, f\"Matched pattern: {pattern}\"\n        \n        # Additional heuristic checks\n        if len(text_lower) &lt; 10:  # Very short messages\n            return True, \"Message too short\"\n            \n        if text_lower.count('@') &gt; 3:  # Too many mentions\n            return True, \"Too many mentions\"\n            \n        if text_lower.count('http') &gt; 2:  # Too many links\n            return True, \"Too many links\"\n            \n        return False, \"OK\"\n\n    async def get_cappers_channel(self):\n        \"\"\"Get the CAPPERS FREE channel entity directly from dialogs\"\"\"\n        logger.info(\"üîç Finding CAPPERS FREE channel in dialogs...\")\n        \n        async for dialog in self.client.iter_dialogs():\n            if 'CAPPERS FREEüíé' in dialog.name:\n                logger.info(f\"‚úÖ Found channel: {dialog.name}\")\n                logger.info(f\"   üÜî Entity ID: {dialog.entity.id}\")\n                logger.info(f\"   üë• Participants: {getattr(dialog.entity, 'participants_count', 'N/A')}\")\n                return dialog.entity\n        \n        raise ValueError(\"CAPPERS FREEüíé channel not found in dialogs\")\n\n    async def start(self):\n        \"\"\"Start the monitor with direct channel access\"\"\"\n        try:\n            self.client = TelegramClient('smart_monitor', self.api_id, self.api_hash)\n            await self.client.start()\n            \n            if self.bot_token:\n                self.bot_client = TelegramClient('bot_session', self.api_id, self.api_hash)\n                await self.bot_client.start(bot_token=self.bot_token)\n            \n            me = await self.client.get_me()\n            logger.info(f\"üß† Smart Monitor Started\")\n            logger.info(f\"ü§ñ Scraping as: {me.first_name}\")\n            \n            # Get the channel entity directly from dialogs\n            self.source_channel_entity = await self.get_cappers_channel()\n            \n            if self.bot_client:\n                bot_me = await self.bot_client.get_me()\n                logger.info(f\"üì¢ Reporting via: {bot_me.first_name}\")\n            \n            logger.info(f\"üéØ Source: {self.source_channel_entity.title}\")\n            logger.info(f\"üìä Reports: {self.report_channel}\")\n            logger.info(f\"‚è∞ Interval: 20 minutes\")\n            logger.info(f\"üõ°Ô∏è  Filtering: {len(self.exclude_patterns)} patterns\")\n            logger.info(\"=\" * 50)\n            \n        except Exception as e:\n            logger.error(f\"‚ùå Failed to start monitor: {e}\")\n            raise\n\n    async def monitor(self):\n        \"\"\"Main monitoring loop using direct entity access\"\"\"\n        try:\n            channel = self.source_channel_entity\n            logger.info(f\"üéØ Monitoring: {channel.title}\")\n            \n            # Start background tasks\n            asyncio.create_task(self.report_scheduler())\n            asyncio.create_task(self.performance_monitor())\n            asyncio.create_task(self.cleanup_task())\n            \n            @self.client.on(events.NewMessage(chats=channel))\n            async def handler(event):\n                if not self.is_running:\n                    return\n                    \n                if event.message.id &lt;= self.last_processed_message_id:\n                    return  # Skip already processed messages\n                    \n                if event.message.media and hasattr(event.message.media, 'photo'):\n                    message_time = event.message.date.replace(tzinfo=None)\n                    seven_days_ago = datetime.now() - timedelta(days=7)\n                    \n                    if message_time &gt;= seven_days_ago:\n                        logger.info(f\"üÜï New post detected at {message_time.strftime('%H:%M:%S')} (ID: {event.message.id})\")\n                        await self.process_message(event.message, channel.title)\n                        self.last_processed_message_id = event.message.id\n                        self.save_state()\n            \n            logger.info(\"üîÑ Monitor running... (Ctrl+C to stop)\")\n            \n            # Wait for shutdown signal\n            await self.shutdown_event.wait()\n            logger.info(\"üõë Shutdown signal received, stopping monitor...\")\n            \n        except Exception as e:\n            logger.error(f\"‚ùå Monitor error: {e}\")\n            raise\n\n    async def cleanup_task(self):\n        \"\"\"Cleanup temporary files and optimize database\"\"\"\n        while self.is_running:\n            await asyncio.sleep(3600)  # Every hour\n            \n            try:\n                # Cleanup old temp files\n                temp_dir = 'data/temp'\n                if os.path.exists(temp_dir):\n                    for file in os.listdir(temp_dir):\n                        file_path = os.path.join(temp_dir, file)\n                        if os.path.getmtime(file_path) &lt; (datetime.now().timestamp() - 3600):  # 1 hour old\n                            os.remove(file_path)\n                            \n                # Optimize database\n                conn = sqlite3.connect('data/sports_picks.db')\n                conn.execute(\"VACUUM\")\n                conn.close()\n                \n                logger.info(\"üßπ Cleanup completed\")\n                \n            except Exception as e:\n                logger.warning(f\"Cleanup error: {e}\")\n\n    async def performance_monitor(self):\n        \"\"\"Monitor system performance\"\"\"\n        while self.is_running:\n            await asyncio.sleep(300)  # Every 5 minutes\n            if self.total_picks_processed &gt; 0:\n                logger.info(f\"üìà Performance: {self.total_picks_processed} total picks, {self.performance_stats['messages_filtered']} filtered\")\n\n    async def report_scheduler(self):\n        \"\"\"Send reports every 20 minutes with enhanced timing\"\"\"\n        while self.is_running:\n            await asyncio.sleep(60)  # Check every minute\n            \n            time_since_last_report = datetime.now() - self.last_report_time\n            if time_since_last_report &gt;= self.report_interval:\n                logger.info(f\"üìä Generating scheduled report... ({time_since_last_report.total_seconds()/60:.1f}m elapsed)\")\n                await self.send_consensus_report()\n                self.last_report_time = datetime.now()\n                self.new_picks_count = 0\n\n    async def send_consensus_report(self):\n        \"\"\"Generate and send consensus report with enhanced pandas analytics\"\"\"\n        try:\n            consensus_data = self.calculate_consensus()\n            \n            if not consensus_data['has_data']:\n                if self.new_picks_count &gt; 0:\n                    message = self.format_no_consensus_report()\n                else:\n                    logger.info(\"ü§´ No new picks - staying silent\")\n                    return\n            else:\n                message = self.format_consensus_report(consensus_data)\n            \n            if self.bot_client and self.report_channel:\n                await self.bot_client.send_message(self.report_channel, message)\n                logger.info(f\"üì¢ Report sent to {self.report_channel}\")\n                \n        except Exception as e:\n            logger.error(f\"‚ùå Error sending report: {e}\")\n\n    def calculate_consensus(self):\n        \"\"\"Enhanced consensus calculation with pandas\"\"\"\n        try:\n            conn = sqlite3.connect('data/sports_picks.db')\n            \n            # Use pandas to read with optimized query\n            query = \"\"\"\n            SELECT sport, teams, pick_type, pick_value, handicapper, confidence, created_at\n            FROM picks \n            WHERE created_at &gt; datetime('now', '-24 hours')\n            AND sport != 'UNKNOWN'\n            AND confidence &gt; 0.3\n            \"\"\"\n            \n            df = pd.read_sql_query(query, conn)\n            conn.close()\n            \n            if df.empty:\n                return {'has_data': False, 'total_picks': 0}\n            \n            # Enhanced analytics with latest pandas\n            df['created_dt'] = pd.to_datetime(df['created_at'])\n            \n            # Calculate consensus with weighted confidence\n            consensus_df = df.groupby(['sport', 'teams', 'pick_type', 'pick_value']).agg({\n                'handicapper': ['nunique', 'count'],\n                'confidence': ['mean', 'std'],\n                'created_dt': 'max'\n            }).reset_index()\n            \n            # Flatten column names\n            consensus_df.columns = ['sport', 'teams', 'pick_type', 'pick_value', \n                                  'unique_handicappers', 'total_picks', \n                                  'avg_confidence', 'confidence_std', 'latest_pick']\n            \n            # Calculate consensus score (weighted by handicappers and confidence)\n            consensus_df['consensus_score'] = (\n                consensus_df['unique_handicappers'] * \n                consensus_df['avg_confidence'] * \n                (1 / (consensus_df['confidence_std'].fillna(0.1) + 0.1))\n            )\n            \n            # Filter for meaningful consensus\n            consensus_df = consensus_df[\n                (consensus_df['unique_handicappers'] &gt;= 2) &amp;\n                (consensus_df['avg_confidence'] &gt;= 0.5)\n            ]\n            \n            # Sort by consensus strength\n            consensus_df = consensus_df.sort_values('consensus_score', ascending=False)\n            \n            # Convert to list of dicts\n            consensus_picks = consensus_df.head(8).to_dict('records')\n            \n            # Additional analytics\n            picks_per_hour = len(df) / 24\n            handicapper_activity = df['handicapper'].value_counts().head(5).to_dict()\n            \n            return {\n                'has_data': len(consensus_picks) &gt; 0,\n                'consensus_picks': consensus_picks,\n                'total_picks': len(df),\n                'unique_handicappers': df['handicapper'].nunique(),\n                'sports_covered': df['sport'].nunique(),\n                'picks_per_hour': picks_per_hour,\n                'top_handicappers': handicapper_activity,\n                'analysis_time': datetime.now().strftime('%H:%M:%S')\n            }\n            \n        except Exception as e:\n            logger.error(f\"‚ùå Pandas analysis error: {e}\")\n            return {'has_data': False, 'total_picks': 0}\n\n    def format_consensus_report(self, data):\n        \"\"\"Enhanced report formatting with analytics\"\"\"\n        report = []\n        report.append(\"üéØ **THE LINE WEAVER - CONSENSUS REPORT**\")\n        report.append(f\"‚è∞ {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n        report.append(\"\")\n        \n        # Enhanced analytics summary\n        report.append(\"üìä **ANALYTICS DASHBOARD**\")\n        report.append(f\"‚Ä¢ üìà Total Picks: **{data['total_picks']}**\")\n        report.append(f\"‚Ä¢ üë• Unique Handicappers: **{data['unique_handicappers']}**\")\n        report.append(f\"‚Ä¢ üèÄ Sports Covered: **{data['sports_covered']}**\")\n        report.append(f\"‚Ä¢ ‚ö° Picks/Hour: **{data['picks_per_hour']:.1f}**\")\n        report.append(f\"‚Ä¢ üî• Strong Consensus: **{len(data['consensus_picks'])}**\")\n        report.append(f\"‚Ä¢ üõ°Ô∏è  Messages Filtered: **{self.performance_stats['messages_filtered']}**\")\n        report.append(\"\")\n        \n        if data['consensus_picks']:\n            report.append(\"üî• **TOP CONSENSUS PICKS**\")\n            report.append(\"\")\n            \n            for i, pick in enumerate(data['consensus_picks'][:5], 1):\n                # Enhanced strength indicators\n                if pick['unique_handicappers'] &gt;= 4:\n                    strength_emoji = \"üü¢üü¢\"\n                    strength_text = \"VERY STRONG\"\n                elif pick['unique_handicappers'] &gt;= 3:\n                    strength_emoji = \"üü¢\"\n                    strength_text = \"STRONG\"\n                else:\n                    strength_emoji = \"üü°\"\n                    strength_text = \"MODERATE\"\n                \n                report.append(f\"{strength_emoji} **{strength_text} CONSENSUS**\")\n                report.append(f\"**{pick['sport']}** ‚Ä¢ {pick['teams']}\")\n                report.append(f\"üéØ **{pick['pick_type']} {pick['pick_value']}**\")\n                report.append(f\"üë• **{pick['unique_handicappers']}** handicappers\")\n                report.append(f\"üìä **{pick['avg_confidence']:.0%}** confidence\")\n                report.append(f\"‚≠ê Score: **{pick['consensus_score']:.1f}**\")\n                \n                if i &lt; len(data['consensus_picks'][:5]):\n                    report.append(\"\")\n        \n        # Top handicappers\n        if data['top_handicappers']:\n            report.append(\"\")\n            report.append(\"üèÜ **TOP HANDICAPPERS**\")\n            for handicapper, count in list(data['top_handicappers'].items())[:3]:\n                report.append(f\"‚Ä¢ **{handicapper}**: {count} picks\")\n        \n        report.append(\"\")\n        report.append(\"_ü§ñ Powered by The Line Weaver v2.2_\")\n        report.append(f\"_üìä Analyzed at {data['analysis_time']}_\")\n        \n        return \"\\n\".join(report)\n\n    def format_no_consensus_report(self):\n        \"\"\"Enhanced no consensus report\"\"\"\n        report = []\n        report.append(\"üéØ **THE LINE WEAVER - ACTIVITY REPORT**\")\n        report.append(f\"‚è∞ {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n        report.append(\"\")\n        report.append(f\"üìä **{self.new_picks_count}** new picks processed\")\n        report.append(f\"üõ°Ô∏è  **{self.performance_stats['messages_filtered']}** messages filtered\")\n        report.append(\"\")\n        report.append(\"‚ÑπÔ∏è **No strong consensus detected**\")\n        report.append(\"\")\n        report.append(\"üí° **System Status:**\")\n        report.append(\"‚Ä¢ ‚úÖ Monitor: Active\")\n        report.append(\"‚Ä¢ ‚úÖ OCR: Operational\") \n        report.append(\"‚Ä¢ ‚úÖ Database: Ready\")\n        report.append(\"‚Ä¢ üõ°Ô∏è  Filtering: Active\")\n        report.append(\"‚Ä¢ üîÑ Consensus: Analyzing...\")\n        report.append(\"\")\n        report.append(\"_ü§ñ Powered by The Line Weaver v2.2_\")\n        return \"\\n\".join(report)\n\n    async def process_message(self, message, channel_name):\n        \"\"\"Enhanced message processing with better OCR and filtering\"\"\"\n        start_time = datetime.now()\n        try:\n            # Get message text for filtering\n            message_text = message.text or \"\"\n            \n            # Check if message should be filtered\n            should_filter, reason = self.should_filter_message(message_text)\n            if should_filter:\n                logger.info(f\"üõ°Ô∏è  Filtered message {message.id}: {reason}\")\n                self.performance_stats['messages_filtered'] += 1\n                self.save_filtered_message(message.id, reason)\n                return\n            \n            temp_file = f\"data/temp/temp_{message.id}_{int(datetime.now().timestamp())}.jpg\"\n            file_path = await message.download_media(file=temp_file)\n            \n            if file_path:\n                async with aiofiles.open(file_path, 'rb') as f:\n                    image_data = await f.read()\n                \n                # Enhanced OCR processing\n                text, ocr_confidence = self.enhanced_ocr_processing(image_data)\n                \n                # Also check OCR text for filtering\n                if text.strip():\n                    should_filter_ocr, ocr_reason = self.should_filter_message(text)\n                    if should_filter_ocr:\n                        logger.info(f\"üõ°Ô∏è  Filtered OCR content {message.id}: {ocr_reason}\")\n                        self.performance_stats['messages_filtered'] += 1\n                        self.save_filtered_message(message.id, f\"OCR: {ocr_reason}\")\n                    else:\n                        picks = self.parse_sports_picks(text)\n                        \n                        if picks:\n                            self.save_picks_to_db(picks, channel_name, message.date, image_data, ocr_confidence, message.id)\n                            self.new_picks_count += len(picks)\n                            self.total_picks_processed += len(picks)\n                            logger.info(f\"‚úÖ Processed {len(picks)} picks (New: {self.new_picks_count}, Total: {self.total_picks_processed})\")\n                        else:\n                            logger.info(\"‚ÑπÔ∏è  No picks found in image\")\n                \n                # Clean up\n                if os.path.exists(file_path):\n                    os.remove(file_path)\n                \n                # Update performance stats\n                processing_time = (datetime.now() - start_time).total_seconds()\n                self.performance_stats['images_processed'] += 1\n                self.performance_stats['avg_processing_time'] = (\n                    self.performance_stats['avg_processing_time'] * 0.9 + processing_time * 0.1\n                )\n                \n        except Exception as e:\n            logger.error(f\"‚ùå Error processing message: {e}\")\n            self.performance_stats['errors_count'] += 1\n\n    def save_filtered_message(self, message_id, reason):\n        \"\"\"Save filtered message to database for tracking\"\"\"\n        try:\n            conn = sqlite3.connect('data/sports_picks.db')\n            cursor = conn.cursor()\n            \n            cursor.execute('''\n                INSERT INTO filtered_messages (message_id, reason)\n                VALUES (?, ?)\n            ''', (message_id, reason))\n            \n            conn.commit()\n            conn.close()\n        except Exception as e:\n            logger.error(f\"‚ùå Error saving filtered message: {e}\")\n\n    def enhanced_ocr_processing(self, image_data):\n        \"\"\"Enhanced OCR with advanced image preprocessing\"\"\"\n        try:\n            image = Image.open(io.BytesIO(image_data))\n            \n            # Advanced image preprocessing for better OCR\n            image = self.advanced_preprocess_image(image)\n            \n            # Multiple OCR attempts with different configurations\n            configs = [\n                '--psm 6 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789@.-+ vsVS()',  # Uniform block with whitelist\n                '--psm 4 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789@.-+ vsVS()',  # Single column\n                '--psm 3',  # Fully automatic page segmentation\n                '--psm 8',  # Single word\n                '--psm 7',  # Single text line\n            ]\n            \n            best_text = \"\"\n            best_confidence = 0.0\n            \n            for config in configs:\n                try:\n                    # Get OCR data with confidence\n                    ocr_data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT, config=config)\n                    \n                    # Calculate average confidence for valid text\n                    confidences = [int(conf) for conf, text in zip(ocr_data['conf'], ocr_data['text']) \n                                 if int(conf) &gt; 0 and text.strip()]\n                    \n                    if confidences:\n                        avg_confidence = sum(confidences) / len(confidences)\n                        text = ' '.join([text for text in ocr_data['text'] if text.strip()])\n                        \n                        if avg_confidence &gt; best_confidence and len(text) &gt; 10:  # Minimum text length\n                            best_confidence = avg_confidence\n                            best_text = text\n                except Exception as e:\n                    continue\n            \n            if best_text:\n                best_text = re.sub(r'\\s+', ' ', best_text).strip()\n                logger.info(f\"üìù OCR: {len(best_text)} chars, Confidence: {best_confidence:.1f}%\")\n            \n            return best_text, best_confidence / 100.0  # Convert to 0-1 scale\n            \n        except Exception as e:\n            logger.error(f\"‚ùå Enhanced OCR Error: {e}\")\n            return \"\", 0.0\n\n    def advanced_preprocess_image(self, image):\n        \"\"\"Advanced image preprocessing for better OCR results\"\"\"\n        # Convert to grayscale\n        if image.mode != 'L':\n            image = image.convert('L')\n        \n        # Enhance contrast with adaptive method\n        enhancer = ImageEnhance.Contrast(image)\n        image = enhancer.enhance(2.5)  # Increased contrast\n        \n        # Enhance sharpness\n        enhancer = ImageEnhance.Sharpness(image)\n        image = enhancer.enhance(3.0)  # Increased sharpness\n        \n        # Apply bilateral filter for noise reduction while preserving edges\n        image = image.filter(ImageFilter.MedianFilter(size=3))\n        \n        # Apply histogram equalization for better contrast\n        image = ImageOps.equalize(image)\n        \n        # Resize for better OCR (if too small or too large)\n        min_size, max_size = 500, 2000\n        current_min = min(image.size)\n        current_max = max(image.size)\n        \n        if current_min &lt; min_size:\n            ratio = min_size / current_min\n            new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))\n            image = image.resize(new_size, Image.Resampling.LANCZOS)\n        elif current_max &gt; max_size:\n            ratio = max_size / current_max\n            new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))\n            image = image.resize(new_size, Image.Resampling.LANCZOS)\n        \n        # Apply additional sharpening after resize\n        image = image.filter(ImageFilter.SHARPEN)\n        \n        return image\n\n    def parse_sports_picks(self, text):\n        \"\"\"Enhanced sports picks parsing for CAPPERS FREE channel\"\"\"\n        picks = []\n        \n        # Clean the text first - remove watermarks and DM prompts\n        cleaned_text = self.clean_cappers_text(text)\n        \n        # Enhanced team recognition\n        nba_teams = {\n            'LAL', 'LAC', 'GSW', 'BOS', 'PHI', 'MIL', 'DEN', 'PHX', 'DAL', 'MIA',\n            'ATL', 'CHI', 'CLE', 'DET', 'HOU', 'IND', 'MEM', 'MIN', 'NOP', 'NYK',\n            'BKN', 'ORL', 'POR', 'SAC', 'SAS', 'OKC', 'UTA', 'WAS', 'TOR', 'CHA'\n        }\n        \n        nfl_teams = {\n            'SF', 'KC', 'PHI', 'DAL', 'BUF', 'BAL', 'MIA', 'DET', 'LAR', 'GB',\n            'CIN', 'SEA', 'PIT', 'LAC', 'MIN', 'NO', 'ATL', 'TB', 'CHI', 'NYJ',\n            'NYG', 'JAX', 'TEN', 'LV', 'ARI', 'NE', 'WAS', 'CAR', 'DEN', 'CLE', 'HOU', 'IND'\n        }\n        \n        # Extract capper name first (they're usually at the beginning)\n        capper = self.extract_capper_name(cleaned_text)\n        \n        # Multiple patterns for team detection\n        patterns = [\n            r'([A-Z]{2,4})\\s+(?:vs|@|-)\\s+([A-Z]{2,4})',\n            r'([A-Z]{2,4})\\s+at\\s+([A-Z]{2,4})',\n            r'([A-Z]{2,4})\\s+v\\s+([A-Z]{2,4})',\n            r'([A-Z]{2,4})\\s+VS\\s+([A-Z]{2,4})',\n        ]\n        \n        for pattern in patterns:\n            matches = re.finditer(pattern, cleaned_text, re.IGNORECASE)\n            for match in matches:\n                team1, team2 = match.groups()\n                team1, team2 = team1.upper(), team2.upper()\n                \n                sport = 'UNKNOWN'\n                if team1 in nba_teams or team2 in nba_teams:\n                    sport = 'NBA'\n                elif team1 in nfl_teams or team2 in nfl_teams:\n                    sport = 'NFL'\n                \n                if sport != 'UNKNOWN':\n                    pick = {\n                        'handicapper': capper,\n                        'sport': sport,\n                        'teams': f\"{team1} vs {team2}\",\n                        'pick_type': self.extract_pick_type(cleaned_text),\n                        'pick_value': self.extract_pick_value(cleaned_text),\n                        'confidence': self.calculate_confidence(cleaned_text, capper)\n                    }\n                    picks.append(pick)\n                    logger.info(f\"üéØ {capper}: {sport} - {team1} vs {team2}\")\n        \n        return picks\n\n    def clean_cappers_text(self, text):\n        \"\"\"Clean CAPPERS FREE specific watermarks and patterns\"\"\"\n        # Remove watermarks\n        patterns_to_remove = [\n            r'DM‚û°Ô∏è@cappersfree‚úÖ',\n            r'@cappersfree',\n            r'cappersfree',\n            r'DM\\s*@\\w+',\n            r'‚û°Ô∏è@\\w+',\n            r'‚úÖ',\n            r'üÜì',\n            r'üíé',\n        ]\n        \n        cleaned = text\n        for pattern in patterns_to_remove:\n            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)\n        \n        return cleaned.strip()\n\n    def extract_capper_name(self, text):\n        \"\"\"Extract capper name from CAPPERS FREE format\"\"\"\n        # Common capper patterns in this channel\n        capper_patterns = [\n            r'^([A-Z][a-z]+[A-Z][a-z]+)',  # CamelCase like KingCap, PorterPicks\n            r'([A-Z][a-z]+[A-Z][a-z]+)\\s',  # CamelCase with space\n            r'Lock:\\s*@?(\\w+)',             # Lock: @Username\n            r'Play:\\s*@?(\\w+)',             # Play: @Username\n            r'Pick:\\s*@?(\\w+)',             # Pick: @Username\n            r'^@(\\w+)',                     # @username at start\n        ]\n        \n        for pattern in capper_patterns:\n            match = re.search(pattern, text)\n            if match:\n                return match.group(1)\n        \n        return 'Unknown'\n\n    def calculate_confidence(self, text, capper):\n        \"\"\"Calculate confidence score based on text quality and capper\"\"\"\n        base_confidence = 0.7\n        \n        # Boost confidence for known cappers\n        known_cappers = ['KingCap', 'PorterPicks', 'LockLineLarry', 'VegasVic']\n        if capper in known_cappers:\n            base_confidence += 0.1\n        \n        # Adjust based on text length and content quality\n        if len(text) &gt; 50:\n            base_confidence += 0.1\n        \n        return min(base_confidence, 0.95)\n\n    def extract_pick_type(self, text):\n        text_lower = text.lower()\n        if any(word in text_lower for word in ['spread', 'line', 'ats']):\n            return 'Spread'\n        elif any(word in text_lower for word in ['moneyline', 'ml']):\n            return 'Moneyline'\n        elif any(word in text_lower for word in ['total', 'over', 'under', 'o/u']):\n            return 'Total'\n        return 'Spread'\n\n    def extract_pick_value(self, text):\n        # Look for spread values with +/-\n        match = re.search(r'([+-]?\\d+\\.?\\d*)', text)\n        return match.group(1) if match else '+0.0'\n\n    def save_picks_to_db(self, picks, channel_name, message_date, image_data, ocr_confidence, message_id):\n        \"\"\"Save picks to database with enhanced data\"\"\"\n        conn = sqlite3.connect('data/sports_picks.db')\n        cursor = conn.cursor()\n        \n        image_hash = hashlib.md5(image_data).hexdigest()\n        \n        for pick in picks:\n            try:\n                cursor.execute('''\n                    INSERT OR IGNORE INTO picks \n                    (handicapper, sport, teams, pick_type, pick_value, confidence, \n                     extracted_date, source_channel, image_hash, ocr_confidence, message_id)\n                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n                ''', (\n                    pick['handicapper'],\n                    pick['sport'],\n                    pick['teams'],\n                    pick['pick_type'],\n                    pick['pick_value'],\n                    pick['confidence'],\n                    message_date.strftime('%Y-%m-%d %H:%M:%S'),\n                    channel_name,\n                    image_hash,\n                    ocr_confidence,\n                    message_id\n                ))\n            except sqlite3.IntegrityError:\n                logger.info(\"üîÑ Duplicate image detected, skipping\")\n            except Exception as e:\n                logger.error(f\"‚ùå Database error: {e}\")\n        \n        conn.commit()\n        conn.close()\n\n    async def graceful_shutdown(self):\n        \"\"\"Perform graceful shutdown\"\"\"\n        logger.info(\"üö¶ Initiating graceful shutdown...\")\n        self.is_running = False\n        self.shutdown_event.set()\n        \n        # Save final state\n        self.save_state()\n        \n        # Disconnect clients\n        if self.client:\n            await self.client.disconnect()\n            logger.info(\"‚úÖ User client disconnected\")\n        if self.bot_client:\n            await self.bot_client.disconnect()\n            logger.info(\"‚úÖ Bot client disconnected\")\n        \n        logger.info(f\"üì¥ Monitor stopped gracefully at {datetime.now().strftime('%H:%M:%S')}\")\n        logger.info(f\"üìà Final Stats: {self.total_picks_processed} total picks processed\")\n        logger.info(f\"üõ°Ô∏è  Messages filtered: {self.performance_stats['messages_filtered']}\")\n\nasync def main():\n    monitor = SmartMonitor()\n    \n    def signal_handler(sig, frame):\n        print(f\"\\nüõë Received shutdown signal ({sig})...\")\n        asyncio.create_task(monitor.graceful_shutdown())\n    \n    # Register signal handlers for graceful shutdown\n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n    \n    try:\n        await monitor.start()\n        await monitor.monitor()\n    except Exception as e:\n        logger.error(f\"‚ùå Fatal error: {e}\")\n    finally:\n        # Ensure graceful shutdown even on unexpected errors\n        if monitor.is_running:\n            await monitor.graceful_shutdown()\n\nif __name__ == \"__main__\":\n    # Create data directory if it doesn't exist\n    os.makedirs('data', exist_ok=True)\n    os.makedirs('data/temp', exist_ok=True)\n    \n    # Check for required dependencies\n    try:\n        import pytesseract\n        # Test if tesseract is available\n        pytesseract.get_tesseract_version()\n    except Exception as e:\n        logger.error(f\"‚ùå Tesseract OCR not available: {e}\")\n        logger.info(\"üí° Please install tesseract-ocr:\")\n        logger.info(\"   Ubuntu: sudo apt-get install tesseract-ocr\")\n        logger.info(\"   macOS: brew install tesseract\")\n        logger.info(\"   Windows: Download from https://github.com/UB-Mannheim/tesseract/wiki\")\n        sys.exit(1)\n    \n    # Run the main async function\n    asyncio.run(main())The key fixes I made:Completed the main() function - Fixed the incomplete finally block that was cut offAdded proper __main__ guard - Ensures the script runs correctly when executed directlyAdded dependency checking - Verifies Tesseract OCR is available before startingFixed graceful shutdown - Now properly calls graceful_shutdown() in all scenariosEnhanced error handling - Better dependency validation and startup checksThe script now:Starts up properly with dependency checksRuns the monitoring loop continuouslyHandles Ctrl+C and system signals gracefullyCleans up resources on shutdownProvides clear error messages if dependencies are missingYou can now run this with python smart_monitor.py and it should work correctly!This response is AI-generated, for reference only."
  },
  {
    "question": "Let me see the entire correct and updated smart_monitor.py",
    "answer": ""
  }
]